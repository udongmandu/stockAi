import streamlit as st
import pandas as pd
import requests
from bs4 import BeautifulSoup
import time
import os
import json
from datetime import datetime, timedelta
import plotly.graph_objects as go
from dotenv import load_dotenv
import re
from sqlalchemy import create_engine, text
from urllib.parse import quote_plus

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# db ì—°ê²°
DB_HOST = os.getenv("DB_HOST")
DB_USER = os.getenv("DB_USER")
DB_PASSWORD = os.getenv("DB_PASSWORD")
DB_PASSWORD_SAFE = quote_plus(DB_PASSWORD)
DB_NAME = os.getenv("DB_NAME")
DB_PORT = os.getenv("DB_PORT", 3306)

engine = create_engine(
    f"mysql+pymysql://{DB_USER}:{DB_PASSWORD_SAFE}@{DB_HOST}:{DB_PORT}/{DB_NAME}",
    echo=True
)

with engine.connect() as conn:
    result = conn.execute(text("SELECT NOW();"))
    print("DB ì—°ê²° ì„±ê³µ âœ… í˜„ì¬ ì‹œê°„:", result.scalar())
    

KRX_FILE = "krx_temp.xls"

# ë‚ ì§œë³„ ì„¹ì…˜ ë‰´ìŠ¤(ì¼ë°˜ ëª¨ë“œ)
NEWS_BY_DATE_URL = "https://finance.naver.com/news/mainnews.naver?&date={date}&page={page}"
CLOVA_API_KEY = os.getenv("CLOVA_API_KEY")
CLOVA_URL = "https://clovastudio.stream.ntruss.com/testapp/v1/chat-completions/HCX-003"

st.set_page_config(page_title="KRX ë‰´ìŠ¤-AI ìë™í™”", layout="centered")
st.title("KRX ìƒì¥ì¢…ëª© ë‰´ìŠ¤ + AI ë¶„ì„ ìë™í™”")

# -------------------- ì„¸ì…˜/ìºì‹œ --------------------
@st.cache_resource
def get_session():
    from requests.adapters import HTTPAdapter
    from urllib3.util.retry import Retry

    s = requests.Session()
    s.headers.update({
        "User-Agent": "Mozilla/5.0",
        "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
    })

    # ì»¤ë„¥ì…˜ í’€/ì¬ì‹œë„
    retry = Retry(
        total=3, backoff_factor=0.3,
        status_forcelist=[429, 500, 502, 503, 504],
        allowed_methods=["GET", "HEAD", "OPTIONS"]
    )
    adapter = HTTPAdapter(max_retries=retry, pool_connections=20, pool_maxsize=20)
    s.mount("http://", adapter)
    s.mount("https://", adapter)
    return s

SESSION = get_session()

@st.cache_data(ttl=3600)
def load_krx_excel(path: str) -> pd.DataFrame:
    try:
        try:
            df = pd.read_excel(path, dtype=str, engine="xlrd")
        except Exception:
            df = pd.read_excel(path, dtype=str)
        df["ì¢…ëª©ì½”ë“œ"] = df["ì¢…ëª©ì½”ë“œ"].apply(lambda x: str(x).zfill(6))
        return df
    except Exception as e:
        raise e

@st.cache_data(ttl=1800)
def crawl_naver_daily_price_cached(stock_code, max_days=60) -> pd.DataFrame:
    base_url = "https://finance.naver.com/item/sise_day.naver"
    all_rows, page = [], 1
    while True:
        params = {"code": stock_code, "page": page}
        res = SESSION.get(base_url, params=params, timeout=10)
        soup = BeautifulSoup(res.content, "lxml")
        table = soup.find("table", class_="type2")
        if not table:
            break
        rows = table.find_all("tr")
        data_rows = [row for row in rows if len(row.find_all("td")) == 7]
        if not data_rows:
            break
        for row in data_rows:
            cols = row.find_all("td")
            date = cols[0].get_text(strip=True).replace(".", "-")
            close = cols[1].get_text(strip=True).replace(",", "")
            open_price = cols[3].get_text(strip=True).replace(",", "")
            high = cols[4].get_text(strip=True).replace(",", "")
            low = cols[5].get_text(strip=True).replace(",", "")
            volume = cols[6].get_text(strip=True).replace(",", "")
            all_rows.append({
                "ë‚ ì§œ": date,
                "ì¢…ê°€": float(close) if close else None,
                "ì‹œê°€": float(open_price) if open_price else None,
                "ê³ ê°€": float(high) if high else None,
                "ì €ê°€": float(low) if low else None,
                "ê±°ë˜ëŸ‰": int(volume) if volume else None
            })
            if len(all_rows) >= max_days:
                break
        if len(all_rows) >= max_days:
            break
        page += 1
    df = pd.DataFrame(all_rows)
    df = df.dropna(subset=["ì¢…ê°€"])
    df["ë‚ ì§œ"] = pd.to_datetime(df["ë‚ ì§œ"])
    df = df.sort_values("ë‚ ì§œ").reset_index(drop=True)
    return df

# í™˜ê²½ë³€ìˆ˜ì—ì„œ API í‚¤ ì½ê¸°
api_key_env = os.getenv("OPENAI_API_KEY")
if "api_key" not in st.session_state:
    st.session_state.api_key = api_key_env

def submit_api_key():
    st.session_state.api_key = st.session_state.api_key_input

if st.session_state.api_key is None:
    st.text_input("API í‚¤ê°€ ìˆëŠ” ENV íŒŒì¼ì„ ë°›ì•„ ì£¼ì„¸ìš”.", type="password", key="api_key_input", on_change=submit_api_key)
else:
    st.success("âœ… ENV íŒŒì¼ ì¸ì‹ ì™„ë£Œ")
    

# ëª¨ë“œ í† ê¸€ ìƒíƒœ
if "specific_mode" not in st.session_state:
    st.session_state.specific_mode = False

# íŒŒì¼ ì²´í¬
file_exists = os.path.isfile(KRX_FILE)
if file_exists and (st.session_state.api_key is not None):
    st.success(f"âœ… '{KRX_FILE}' íŒŒì¼ì´ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
else:
    if not file_exists:
        st.warning(
            "âŒ '{KRX_FILE}' íŒŒì¼ì´ í´ë”ì— ì—†ìŠµë‹ˆë‹¤.\n"
            "https://kind.krx.co.kr/corpgeneral/corpList.do?method=download&searchType=13\n"
            "ìœ„ ì£¼ì†Œì—ì„œ íŒŒì¼ ë°›ì€ í›„\n"
            "íŒŒì¼ -> ë‚´ë³´ë‚´ê¸° -> íŒŒì¼ í˜•ì‹ ë³€ê²½ -> Excel 97 - 2003 í†µí•© ë¬¸ì„œë¡œ ë‚´ë³´ë‚´ê¸°\n"
            "'krx_temp.xls'ë¡œ ì´ë¦„ì„ ë³€ê²½í•˜ì—¬ ì´ íŒŒì´ì¬ íŒŒì¼ê³¼ ê°™ì€ í´ë”ì— ìœ„ì¹˜ì‹œì¼œ ì£¼ì„¸ìš”."
        )
    if st.session_state.api_key is None:
        st.warning("âŒ OpenAI API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")

# KRX ì¢…ëª© ë¡œë”©
if file_exists:
    try:
        df_stocklist = load_krx_excel(KRX_FILE)
        stock_names = sorted(set(df_stocklist["íšŒì‚¬ëª…"].dropna()))
    except Exception as e:
        st.error(f"ì—‘ì…€ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")
        stock_names = []
else:
    stock_names = []

def get_code_from_name(stock_name):
    try:
        matched_rows = df_stocklist[df_stocklist["íšŒì‚¬ëª…"] == stock_name]
        if not matched_rows.empty:
            code = matched_rows.iloc[0]["ì¢…ëª©ì½”ë“œ"]
            return str(code).zfill(6)
        return None
    except Exception:
        return None

# ---------- ë‚ ì§œ íŒŒì‹± (ì¼ë°˜ ëª¨ë“œ í•„ìš” ì‹œ) ----------
def parse_news_date(dt_text: str) -> datetime.date:
    raw = (dt_text or "").strip().replace(".", "-")
    token = raw.split()[0]
    parts = token.split("-")
    if len(parts) == 2:
        year = datetime.today().year
        token = f"{year}-{parts[0].zfill(2)}-{parts[1].zfill(2)}"
    return datetime.strptime(token, "%Y-%m-%d").date()

# ---------- íšŒì‚¬ë‰´ìŠ¤ ë‚ ì§œ íŒŒì‹± ----------
def parse_company_news_date(s: str) -> datetime.date:
    s = (s or "").strip()
    s = s.replace("-", ".").replace("/", ".")
    token = s.split()[0]
    return datetime.strptime(token, "%Y.%m.%d").date()

# ---------- í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬(ê³µë°± ì œê±° + ì†Œë¬¸ì) ----------
def normalize_text(s: str) -> str:
    if not isinstance(s, str):
        return ""
    return s.replace(" ", "").lower()

# ---------- ê³µí†µ ìœ í‹¸ ----------
def find_stock_in_text(text, stock_names_set):
    if not isinstance(text, str):
        return None
    for name in stock_names_set:
        if name in text:
            return name
    return None

def classify_news(title, summary):
    news_text = f"{title} {summary}"
    prompt = f"""
ì•„ë˜ ë‰´ìŠ¤ë¥¼ ë³´ê³  JSON í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•´ë¼.

í˜•ì‹:
{{
  "stock_nm": "ë‰´ìŠ¤ì—ì„œ ì§ì ‘ì ìœ¼ë¡œ ì§€ëª©í•˜ê±°ë‚˜ ì˜í–¥ì„ ë°›ì„ ì£¼ì‹ëª… (ì˜ˆ: ì‚¼ì„±ì „ì, í˜„ëŒ€ì°¨, LGí™”í•™). 
               íŠ¹ì • ì¢…ëª©ì´ ì•„ë‹ˆë¼ ì „ì²´ ì‹œì¥ì— ì˜í–¥ì„ ì¤€ë‹¤ë©´ 'ì „ì²´'",
  "category": "ê´€ë ¨ ì‚°ì—… ì¹´í…Œê³ ë¦¬ (ì˜ˆ: ë°˜ë„ì²´, ìë™ì°¨, ìŒì‹ë£Œ, ê¸ˆìœµ, IT, í—¬ìŠ¤ì¼€ì–´ ë“±)",
  "tag": "í˜¸ì¬ or ì•…ì¬ or ì¤‘ë¦½",
  "description": "í•œ ë¬¸ì¥ ì„¤ëª…",
  "power": 0~100 ì‚¬ì´ ì •ìˆ˜ (ë‰´ìŠ¤ê°€ í•´ë‹¹ ì¢…ëª©/ì‹œì¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ë ¥ ê°•ë„, ìˆ«ìê°€ í´ìˆ˜ë¡ ì˜í–¥ í¼)
}}

ë‰´ìŠ¤: {news_text}
"""
    headers = {
        "Authorization": f"Bearer {CLOVA_API_KEY}",
        "Content-Type": "application/json"
    }
    payload = {
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0.2,
        "topP": 0.8,
        "maxTokens": 512
    }

    try:
        res = requests.post(CLOVA_URL, headers=headers, json=payload, timeout=30)
        res.raise_for_status()
        answer = res.json()["result"]["message"]["content"].strip()
        return json.loads(answer)
    except Exception as e:
        return {
            "stock_nm": "ë¶„ì„ë¶ˆê°€",
            "category": "ê¸°íƒ€",
            "tag": "ì¤‘ë¦½",
            "description": f"CLOVA ì˜¤ë¥˜: {e}",
            "power": 0
        }


def safe_float(val):
    try:
        return float(str(val).replace(",", ""))
    except Exception:
        return None

def crawl_naver_daily_price(stock_code, max_days=60):
    return crawl_naver_daily_price_cached(stock_code, max_days=max_days)

def get_existing_news(engine, title, stock_nm):
    sql = text("SELECT id, stock_nm, tag, category, description, date FROM NEWS_DATA WHERE title = :title AND stock_nm = :stock_nm")
    with engine.connect() as conn:
        row = conn.execute(sql, {"title": title, "stock_nm": stock_nm}).fetchone()
        if row:
            return dict(row._mapping)
    return None


def insert_news_to_db(engine, title, stock_nm, tag, category, description, power, date_str):
    sql = text("""
        INSERT INTO NEWS_DATA (title, stock_nm, tag, category, description, power, date)
        VALUES (:title, :stock_nm, :tag, :category, :description, :power, :date)
    """)
    with engine.begin() as conn:
        conn.execute(sql, {
            "title": title,
            "stock_nm": stock_nm,
            "tag": tag,
            "category": category,
            "description": description,
            "power": power,
            "date": date_str
        })


def plot_bollinger_20day(df_price, stock_name, stock_code, key=None, news_dict=None):
    if len(df_price) < 20:
        st.warning(f"{stock_name} ì‹œì„¸ ë°ì´í„°ê°€ ë¶€ì¡±í•´ ë³¼ë¦°ì € ë°´ë“œë¥¼ ê·¸ë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    window = 20
    df_price[f"MA{window}"] = df_price["ì¢…ê°€"].rolling(window=window).mean()
    df_price[f"STD{window}"] = df_price["ì¢…ê°€"].rolling(window=window).std()
    df_price[f"Upper{window}"] = df_price[f"MA{window}"] + 2 * df_price[f"STD{window}"]
    df_price[f"Lower{window}"] = df_price[f"MA{window}"] - 2 * df_price[f"STD{window}"]
    dates = df_price["ë‚ ì§œ"]; max_date = dates.max(); min_date = dates.min()
    st.write(f"### {stock_name} 20ì¼ ë³¼ë¦°ì € ë°´ë“œ")
    st.markdown(f"[ì£¼ê°€ ìƒì„¸ ë³´ê¸°](https://finance.naver.com/item/main.nhn?code={stock_code})", unsafe_allow_html=True)
    fig = go.Figure(
        data=[
            go.Scatter(x=dates, y=df_price["ì¢…ê°€"], mode="lines", name="ì¢…ê°€"),
            go.Scatter(x=dates, y=df_price[f"MA{window}"], mode="lines", name=f"MA{window}"),
            go.Scatter(x=dates, y=df_price[f"Upper{window}"], mode="lines", name="ìƒë‹¨ ë°´ë“œ"),
            go.Scatter(x=dates, y=df_price[f"Lower{window}"], mode="lines", name="í•˜ë‹¨ ë°´ë“œ", fill="tonexty", fillcolor="rgba(200,200,200,0.2)"),
        ],
        layout=go.Layout(
            xaxis_title="ë‚ ì§œ", yaxis_title="ê°€ê²©",
            xaxis=dict(range=[min_date, max_date + pd.Timedelta(days=3)], fixedrange=True),
            yaxis=dict(autorange=True, fixedrange=True),
            dragmode="pan"
        )
    )
    if news_dict:
        for nd, news_list in news_dict.items():
            try:
                nd_dt = pd.to_datetime(nd)
                price_row = df_price[df_price["ë‚ ì§œ"] == nd_dt]
                if not price_row.empty:
                    price = price_row.iloc[0]["ì¢…ê°€"]
                    news_text = "<br>".join([n["title"] for n in news_list])
                    fig.add_trace(go.Scatter(
                        x=[nd_dt], y=[price], mode="markers+text",
                        marker=dict(color="orange", size=7, symbol="circle"),
                        text=["ğŸ“°"], textposition="top center",
                        name=f"ë‰´ìŠ¤({nd})", hoverinfo="text", hovertext=news_text, showlegend=False
                    ))
            except Exception:
                pass
    st.plotly_chart(fig, use_container_width=True, key=key)

# ------------------- ëª¨ë“œ ì „í™˜ ë²„íŠ¼ (ë‹¨ì¼) -------------------
toggle_label = "â†© ì¼ë°˜ ëª¨ë“œë¡œ ì „í™˜" if st.session_state.specific_mode else "ğŸ“° íŠ¹ì • ê¸°ì‚¬ë§Œ ë³´ê¸° (BETA)"
if st.button(toggle_label):
    st.session_state.specific_mode = not st.session_state.specific_mode

# ------------------- ì†ë„ ê°œì„  ë„ìš°ë¯¸ -------------------
@st.cache_data(ttl=600)
def get_last_page_for_date(date_param: str) -> int:
    url = NEWS_BY_DATE_URL.format(date=date_param, page=1)
    res = SESSION.get(url, timeout=10)
    soup = BeautifulSoup(res.content, "lxml")

    last_link = soup.select_one("td.pgRR a")
    if last_link and last_link.get("href"):
        m = re.search(r"page=(\d+)", last_link["href"])
        if m:
            return int(m.group(1))

    pages = []
    for a in soup.select("table.Nnavi a"):
        txt = (a.get_text(strip=True) or "").strip()
        if txt.isdigit():
            pages.append(int(txt))
    return max(pages) if pages else 1

def parse_mainnews_page(html_bytes, fallback_date_str: str, stock_key_norm: str):
    """
    mainnews í•œ í˜ì´ì§€ì—ì„œ í•´ë‹¹ ì¢…ëª© í‚¤ì›Œë“œê°€ ì œëª©/ìš”ì•½ì— ìˆìœ¼ë©´ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜
    """
    soup = BeautifulSoup(html_bytes, "lxml")
    items = soup.select("ul.newsList > li")
    results = []
    if not items:
        return results

    for li in items:
        a = li.select_one("dd.articleSubject a")
        sm = li.select_one("dd.articleSummary")
        if not a or not sm:
            continue

        title = a.get_text(strip=True)
        href = a.get("href", "")
        link = href if href.startswith("http") else "https://finance.naver.com" + href
        summary = sm.get_text(" ", strip=True)

        dt_tag = sm.select_one("span.wdate")
        if dt_tag:
            news_date_only = dt_tag.get_text(strip=True).split(" ")[0].replace(".", "-")
        else:
            news_date_only = fallback_date_str

        title_key = normalize_text(title)
        summary_key = normalize_text(summary)
        if (stock_key_norm in title_key) or (stock_key_norm in summary_key):
            results.append({
                "ì¢…ëª©ëª…": None,
                "ë‰´ìŠ¤": title,
                "ë§í¬": link,
                "ìš”ì•½": summary,
                "ë‰´ìŠ¤ë‚ ì§œ": news_date_only,
            })
    return results


# ------------------- ëª¨ë“œë³„ UI/ë¡œì§ -------------------
if "is_running" not in st.session_state:
    st.session_state.is_running = False

# ------------------- ì±—ë´‡ ----------------------------
# --- ì±—ë´‡ ë²„íŠ¼ ìœ„ì¹˜ (CSS) ---

if "chat_open" not in st.session_state:
    st.session_state.chat_open = False
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# ë²„íŠ¼ í‘œì‹œ
if st.button("ì±—ë´‡ì—ê²Œ ë¬¼ì–´ë³´ê¸°", key="chat_btn"):
    st.session_state.chat_open = not st.session_state.chat_open

# --- ì±—ë´‡ ì°½ ---
if st.session_state.chat_open:
    st.sidebar.title("RAG ì±—ë´‡")

    chat_container = st.sidebar.container()
    input_container = st.sidebar.container()

    # 1) ëŒ€í™” ë©”ì‹œì§€ í‘œì‹œ
    with chat_container:
        for msg in st.session_state.chat_history:
            with st.chat_message(msg["role"]):
                st.markdown(msg["content"])

    # 2) ì…ë ¥ì°½ì„ í•­ìƒ ì•„ë˜ ê³ ì •
    with input_container:
        question = st.chat_input("ë‰´ìŠ¤/ì£¼ì‹ ì§ˆë¬¸í•˜ê¸°")

    if question:
        with chat_container:
            with st.chat_message("user"):
                st.markdown(question)

        st.session_state.chat_history.append({"role": "user", "content": question})

        # ë¡œë”© ì¤‘ ë©”ì‹œì§€ ë¨¼ì € ì¶œë ¥
        with chat_container:
            with st.chat_message("assistant"):
                loading_placeholder = st.empty()
                loading_placeholder.markdown("ë¡œë”© ì¤‘.....")

        # --- CLOVA ì§ˆë¬¸ í•´ì„ ---
        parse_prompt = f"""
        ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µí•´ë¼ ë¬´ì¡°ê±´.
        í•œêµ­ì–´ ê·¸ëŒ€ë¡œ ì£¼ì‹ëª…ì„ ì‚¬ìš©í•´ì•¼ í•˜ê³ , ë§Œì•½ ì´ë¦„ì´ ì´ìƒí•˜ë‹¤ ì‹¶ìœ¼ë©´ ì˜ˆì¸¡í•´ ì˜ˆë¥¼ë“¤ì–´
        "ì‚¼ì „" ì´ë¼ê³  í•˜ë©´ "ì‚¼ì„±ì „ì" ê² êµ¬ë‚˜ ì´ëŸ°ì‹ìœ¼ë¡œ, ì „ì²´ì ìœ¼ë¡œ ë¬¼ì–´ë³´ëŠ”ê±° ê°™ìœ¼ë©´ "ì „ì²´"ë¡œ ê²€ìƒ‰ í•˜ë©´ ë‹¤ ë‚˜ì˜¤ë‹ˆê¹Œ í•„ìš”í•˜ë©´ ì´ê±¸ë¡œ í•´.
        {{
          "stock": "ì¡°íšŒí•  ì£¼ì‹ëª… (ì˜ˆ: ì‚¼ì„±ì „ì, í˜„ëŒ€ì°¨, ì „ì²´)",
          "days": ìµœê·¼ Nì¼ (ì •ìˆ˜, ì—†ìœ¼ë©´ ê¸°ë³¸ 30),
          "limit": ìµœëŒ€ ê²°ê³¼ ê°œìˆ˜ (ì—†ìœ¼ë©´ 20)
        }}
        ì§ˆë¬¸: "{question}"
        """
        headers = {"Authorization": f"Bearer {CLOVA_API_KEY}", "Content-Type": "application/json"}
        payload = {"messages": [{"role": "user", "content": parse_prompt}]}
        res = requests.post(CLOVA_URL, headers=headers, json=payload, timeout=30)
        parsed = json.loads(res.json()["result"]["message"]["content"])

        stock = parsed.get("stock", "ì „ì²´")
        days = int(parsed.get("days", 30))
        limit = int(parsed.get("limit", 20))

        # --- DB ê²€ìƒ‰ ---
        with engine.connect() as conn:
            sql = text("""
                SELECT title, stock_nm, tag, category, description, power, date
                FROM NEWS_DATA
                WHERE (:stock = 'ì „ì²´' OR stock_nm = :stock)
                  AND date >= :start_date
                ORDER BY power DESC, date DESC
                LIMIT :limit
            """)
            rows = conn.execute(sql, {
                "stock": stock,
                "start_date": (datetime.today() - timedelta(days=days)).strftime("%Y-%m-%d"),
                "limit": limit
            }).fetchall()

        if rows:
            df = pd.DataFrame(rows, columns=["title","stock_nm","tag","category","description","power","date"])
            context = "\n".join([
                f"{r['date']} [{r['stock_nm']}] {r['title']} "
                f"({r['tag']}, {r['category']}, power={r['power']}): {r['description']}"
                for r in df.to_dict(orient="records")
            ])
        else:
            context = "ê´€ë ¨ ë‰´ìŠ¤ ì—†ìŒ."

        answer_prompt = f"""
        ì‚¬ìš©ìê°€ "{question}" ë¼ê³  ë¬¼ì—ˆìŠµë‹ˆë‹¤.
        ì•„ë˜ëŠ” ìµœê·¼ {days}ì¼ ë™ì•ˆ {stock} ê´€ë ¨ ë‰´ìŠ¤ ë°ì´í„°ì…ë‹ˆë‹¤.
        ë°˜ë“œì‹œ ì´ ë°ì´í„°ë¥¼ ê·¼ê±°ë¡œ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•´ì„œ ë‹µë³€í•˜ì„¸ìš”.
        ë°ì´í„°ì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ê³  'ê´€ë ¨ ë‰´ìŠ¤ ì—†ìŒ'ì´ë¼ê³  ë§í•˜ì„¸ìš”.
        power ê°’ì€ í•´ë‹¹ ë‰´ìŠ¤ì˜ ì˜í–¥ë ¥ì„ ì˜ˆì¸¡í•œ ê°’ì´ë‹ˆê¹Œ ë§Œì•½ ì§ˆë¬¸ì´ ì˜ˆì¸¡í•´ì¤˜ í˜¹ì€ ì¶”ì²œí•´ì¤˜ ì´ëŸ°ë‚´ìš©ì´ë©´ ì´ power ê°’ì„ ì°¸ì¡°í•˜ê³ 
        ê°€ì¥ ì˜í–¥ì„ ì¤€ ê¸°ì‚¬ë¥¼ ë³´ì—¬ì£¼ëŠ” ê²ƒë„ ì¢‹ì•„
        ë°ì´í„°:
        {context}
        """
        payload = {
            "messages": [{"role": "user", "content": answer_prompt}],
            "temperature": 0.4,
            "topP": 0.8,
            "maxTokens": 512
        }
        res = requests.post(CLOVA_URL, headers=headers, json=payload, timeout=30)
        answer = res.json()["result"]["message"]["content"]

        # --- ë¡œë”© ë©”ì‹œì§€ êµì²´ ---
        loading_placeholder.markdown(answer)
        st.session_state.chat_history.append({"role": "assistant", "content": answer})



# ===== íŠ¹ì • ê¸°ì‚¬ë§Œ ë³´ê¸° ëª¨ë“œ (ë‚ ì§œ ì„¹ì…˜ ì „ì²´ ìˆœíšŒ + ì¢…ëª©ëª… í•„í„°) =====
if st.session_state.specific_mode:
    st.subheader("ğŸ“° íŠ¹ì • ê¸°ì‚¬ë§Œ ë³´ê¸°")

    days_to_fetch = st.number_input("ê°€ì ¸ì˜¬ ë‰´ìŠ¤ ì¼ì (ìµœê·¼ Nì¼)", min_value=1, max_value=100, value=30, step=1)
    selected_stocks = st.multiselect(
        "ì¢…ëª© ì„ íƒ (ë³µìˆ˜ ì„ íƒ ê°€ëŠ¥, ê²€ìƒ‰ ê°€ëŠ¥)",
        options=stock_names,
        default=stock_names[:0] if stock_names else []
    )

    can_run_specific = file_exists and (len(selected_stocks) > 0)
    run_specific = st.button("ğŸ” ë‰´ìŠ¤ ê²€ìƒ‰ (íŠ¹ì • ê¸°ì‚¬ë§Œ ë³´ê¸°) ì‹¤í–‰", disabled=not can_run_specific)

    def crawl_mainnews_by_dates_for_stocks(stock_names, days: int = 30, max_pages_per_day: int = 200):
        results = []
        today = datetime.today().date()

        stock_norm_to_orig = {}
        stock_norm_set = set()
        for nm in stock_names or []:
            sn = normalize_text(nm)
            if sn:
                stock_norm_set.add(sn)
                stock_norm_to_orig.setdefault(sn, nm)

        if not stock_norm_set:
            return results

        pattern = re.compile("|".join(map(re.escape, stock_norm_set)))

        for i in range(days):
            d = today - timedelta(days=i)
            date_param = d.strftime("%Y%m%d")

            page = 1
            max_page = None

            while True:
                url = NEWS_BY_DATE_URL.format(date=date_param, page=page)
                try:
                    res = SESSION.get(url, timeout=10)
                    soup = BeautifulSoup(res.content, "lxml")
                except Exception as e:
                    st.warning(f"[{date_param}] ì£¼ìš”ë‰´ìŠ¤ ìš”ì²­ ì‹¤íŒ¨(page {page}): {e}")
                    break

                if page == 1:
                    nav_nums = [
                        int(a.get_text(strip=True))
                        for a in soup.select("table.Nnavi a")
                        if a.get_text(strip=True).isdigit()
                    ]
                    max_page = max(nav_nums) if nav_nums else 1

                items = soup.select("ul.newsList > li")
                if not items:
                    soup.decompose()
                    break

                for li in items:
                    a = li.select_one("dd.articleSubject a")
                    sm = li.select_one("dd.articleSummary")
                    if not (a and sm):
                        continue

                    title = a.get_text(strip=True)
                    href = a.get("href", "")
                    link = href if href.startswith("http") else "https://finance.naver.com" + href
                    summary = sm.get_text(" ", strip=True)

                    dt_tag = sm.select_one("span.wdate")
                    if dt_tag:
                        news_date_only = dt_tag.get_text(strip=True).split(" ")[0].replace(".", "-")
                    else:
                        news_date_only = d.strftime("%Y-%m-%d")

                    tkey = normalize_text(title)
                    skey = normalize_text(summary)
                    m = pattern.search(tkey) or pattern.search(skey)
                    if m:
                        hit_stock = stock_norm_to_orig.get(m.group(0))
                        if hit_stock:
                            results.append({
                                "ì¢…ëª©ëª…": hit_stock,
                                "ë‰´ìŠ¤": title,
                                "ë§í¬": link,
                                "ìš”ì•½": summary,
                                "ë‰´ìŠ¤ë‚ ì§œ": news_date_only,
                            })

                soup.decompose()
                page += 1

                if max_page is not None and page > max_page:
                    break
                if page > max_pages_per_day:
                    break

        # ì¤‘ë³µ ì œê±° + ì •ë ¬
        results = list({(r["ì¢…ëª©ëª…"], r["ë‰´ìŠ¤"], r["ë‰´ìŠ¤ë‚ ì§œ"]): r for r in results}.values())
        results.sort(key=lambda x: (x["ë‰´ìŠ¤ë‚ ì§œ"], x["ì¢…ëª©ëª…"], x["ë‰´ìŠ¤"]))
        return results

    if run_specific:
        st.info(f"ì„ íƒ ì¢…ëª©: {', '.join(selected_stocks)} / ìµœê·¼ {int(days_to_fetch)}ì¼ ê¸°ì‚¬ ìˆ˜ì§‘ ì¤‘â€¦ (ê¸°ì‚¬ì˜ ê°œìˆ˜ê°€ ë§ì„ ìˆ˜ë¡ ì¢€ ê±¸ë ¤ìš©)")
        st.info(f"ì‹œì‘ ë²„íŠ¼ì„ ì—°ì†í•´ì„œ ëˆ„ë¥´ì§€ ë§ˆì„¸ìš” ì§€ê¸ˆ ì‹¤í–‰ì¤‘ì…ë‹ˆë‹¤.")
        news_list = crawl_mainnews_by_dates_for_stocks(selected_stocks, days=int(days_to_fetch))

        if len(news_list) == 0:
            st.warning("í•´ë‹¹ ê¸°ê°„ì— ì„ íƒí•œ ì¢…ëª©ëª…ì´ í¬í•¨ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            df_sel = pd.DataFrame(news_list).drop_duplicates(["ì¢…ëª©ëª…", "ë‰´ìŠ¤"])
            st.write("### ğŸ“„ ìˆ˜ì§‘ ê¸°ì‚¬")
            st.dataframe(df_sel[["ë‰´ìŠ¤ë‚ ì§œ", "ì¢…ëª©ëª…", "ë‰´ìŠ¤", "ë§í¬", "ìš”ì•½"]], use_container_width=True)

            # ì¢…ëª©ë³„ ì°¨íŠ¸
            st.write(f"## ğŸ—“ ì„ íƒ ì¢…ëª© ìµœê·¼ {int(days_to_fetch)}ì¼ ë‰´ìŠ¤ ì°¨íŠ¸")
            for stock in df_sel["ì¢…ëª©ëª…"].dropna().unique():
                code = get_code_from_name(stock)
                if not code:
                    st.warning(f"{stock} ì¢…ëª©ì½”ë“œ ì—†ìŒ, ì°¨íŠ¸ ìƒëµ")
                    continue
                try:
                    df_price = crawl_naver_daily_price(code, max_days=60)
                except Exception as e:
                    st.warning(f"{stock} ì‹œì„¸ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                    continue

                # í•´ë‹¹ ì¢…ëª© ë‰´ìŠ¤ë§Œ ë§¤í•‘
                date_news_map = {}
                for _, nrow in df_sel[df_sel["ì¢…ëª©ëª…"] == stock].iterrows():
                    dt = nrow["ë‰´ìŠ¤ë‚ ì§œ"]
                    date_news_map.setdefault(dt, [])
                    if not any(n["title"] == nrow["ë‰´ìŠ¤"] for n in date_news_map[dt]):
                        date_news_map[dt].append({"title": nrow["ë‰´ìŠ¤"], "link": nrow["ë§í¬"]})

                plot_bollinger_20day(
                    df_price, stock, code,
                    key=f"bollinger_specific_{code}_{days_to_fetch}",
                    news_dict=date_news_map
                )

# ===== ì¼ë°˜ ëª¨ë“œ =====
else:
    news_count = st.number_input("ê°€ì ¸ì˜¬ ë‰´ìŠ¤ ê¸°ì‚¬ ê°œìˆ˜ ì…ë ¥ (í‘œì‹œìš©)", min_value=1, max_value=200, value=20, step=1)
    today_only = st.checkbox("ê¸ˆì¼ ê¸°ì‚¬ë§Œ", value=True)

    start_btn_disabled = not (file_exists and st.session_state.api_key is not None)
    start = st.button("ğŸš€ ì‹œì‘ (ê¸°ë³¸ ëª¨ë“œ)", disabled=start_btn_disabled or st.session_state.is_running)

    def color_news(tag):
        if tag == "í˜¸ì¬":
            return "color: red; font-weight: bold;"
        elif tag == "ì•…ì¬":
            return "color: blue; font-weight: bold;"
        elif tag == "ì¤‘ë¦½":
            return "color: gray;"
        else:
            return ""

    # -------- ì¼ë°˜ ëª¨ë“œ: ë©”ì¸ë‰´ìŠ¤ ì „ì²´ í˜ì´ì§€ ìˆœíšŒ í¬ë¡¤ëŸ¬ --------
    def crawl_mainnews_all_pages(stock_names, today_only=True, max_pages=200):
        results = []
        stock_set = set(stock_names)
        today_str = datetime.today().strftime("%Y-%m-%d")

        page = 1
        while True:
            news_url = f"https://finance.naver.com/news/mainnews.naver?&page={page}"
            try:
                res = SESSION.get(news_url, timeout=10)
                soup = BeautifulSoup(res.content, "lxml")
            except Exception as e:
                st.error(f"ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§ ì˜¤ë¥˜ (í˜ì´ì§€ {page}): {e}")
                return results

            items = soup.select("ul.newsList > li")
            if not items:
                return results

            hit_non_today = False

            for li in items:
                a = li.select_one("dd.articleSubject a")
                sm = li.select_one("dd.articleSummary")
                dt_tag = li.select_one("dd.articleSummary span.wdate")
                if not (a and dt_tag):
                    continue

                # ë‚ ì§œ
                news_date_only = dt_tag.get_text(strip=True).split(" ")[0].replace(".", "-")

                if today_only and news_date_only != today_str:
                    hit_non_today = True
                    continue

                # ì œëª©/ìš”ì•½
                title = a.get_text(strip=True)
                link = a.get("href", "")
                link = link if link.startswith("http") else "https://finance.naver.com" + link
                summary = sm.get_text(" ", strip=True) if sm else ""

                # ì œëª©ê³¼ ìš”ì•½ ë‘˜ ë‹¤ ê²€ì‚¬
                stock = find_stock_in_text(title, stock_set) or find_stock_in_text(summary, stock_set)
                if stock:
                    results.append({
                        "ì¢…ëª©ëª…": stock,
                        "ë‰´ìŠ¤": title,
                        "ë§í¬": link,
                        "ìš”ì•½": summary,
                        "ë‰´ìŠ¤ë‚ ì§œ": news_date_only
                    })

            if today_only and hit_non_today:
                return results

            page += 1
            if page > max_pages:
                return results

    if start:
        st.session_state.is_running = True
        st.info(f"ê¸°ì‚¬ ìˆ˜ì§‘ ì¤‘â€¦ (ê¸°ì‚¬ì˜ ê°œìˆ˜ê°€ ë§ì„ ìˆ˜ë¡ ì¢€ ê±¸ë ¤ìš©)")
        st.info(f"ì‹œì‘ ë²„íŠ¼ì„ ì—°ì†í•´ì„œ ëˆ„ë¥´ì§€ ë§ˆì„¸ìš” ì§€ê¸ˆ ì‹¤í–‰ì¤‘ì…ë‹ˆë‹¤.")
        news_results = crawl_mainnews_all_pages(
            stock_names=stock_names,
            today_only=today_only,
            max_pages=200,
        )

        if len(news_results) == 0:
            st.warning("ë‰´ìŠ¤ì—ì„œ ìƒì¥ì¢…ëª©ëª…ì´ í¬í•¨ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            st.session_state.is_running = False
            st.stop()
            
        news_results_display = news_results[:news_count]
        news_df = pd.DataFrame(news_results_display).drop_duplicates(["ì¢…ëª©ëª…", "ë‰´ìŠ¤"])

        openai_api_key = st.session_state.api_key

        news_df["ë‰´ìŠ¤íŒë³„"] = ""
        news_df["AIì„¤ëª…"] = ""
        today_str = datetime.today().strftime("%Y-%m-%d")

        for idx, row in news_df.iterrows():
            existing = get_existing_news(engine, row["ë‰´ìŠ¤"], row["ì¢…ëª©ëª…"])
            if existing:
                news_df.at[idx, "ë‰´ìŠ¤íŒë³„"] = existing["tag"]
                news_df.at[idx, "AIì„¤ëª…"] = existing["description"]
                news_df.at[idx, "ì¹´í…Œê³ ë¦¬"] = existing["category"]
                st.info(f"[DB] {row['ì¢…ëª©ëª…']}: {row['ë‰´ìŠ¤']} => {existing['tag']}")
            else:
                ai_result = classify_news(row["ë‰´ìŠ¤"], row["ìš”ì•½"])
                news_df.at[idx, "ë‰´ìŠ¤íŒë³„"] = ai_result["tag"]
                news_df.at[idx, "AIì„¤ëª…"] = ai_result["description"]
                news_df.at[idx, "ì¹´í…Œê³ ë¦¬"] = ai_result["category"]

                insert_news_to_db(engine,row["ë‰´ìŠ¤"],row["ì¢…ëª©ëª…"],ai_result["tag"],ai_result["category"],ai_result["description"],ai_result["power"],row["ë‰´ìŠ¤ë‚ ì§œ"])
                st.info(f"[AIë¶„ì„]{row['ì¢…ëª©ëª…']}: {row['ë‰´ìŠ¤']} => {ai_result['tag']}")
                time.sleep(0.05)

        # ì¬ë¬´
        finance_results = []
        for stock in set(news_df["ì¢…ëª©ëª…"]):
            code = get_code_from_name(stock)
            if code is None:
                st.warning(f"{stock} ì¢…ëª©ì½”ë“œ ë¯¸ë°œê²¬, ì¬ë¬´ ë°ì´í„° ìƒëµ")
                continue
            url = f"https://finance.naver.com/item/main.nhn?code={code}"
            try:
                res = SESSION.get(url, timeout=10)
                soup = BeautifulSoup(res.content, "lxml")
            except Exception as e:
                st.warning(f"{stock} ì¬ë¬´ ë°ì´í„° ìš”ì²­ ì‹¤íŒ¨: {e}")
                continue

            price = "N/A"
            try:
                price_tag = soup.select_one("p.no_today span.blind")
                if price_tag:
                    price = price_tag.get_text(strip=True).replace(",", "")
            except Exception:
                pass

            per = pbr = roe = eps = "N/A"
            table = soup.select_one("table.tb_type1_ifrs")
            if table:
                try:
                    rows = table.select("tr")
                    for r in rows:
                        th = r.find("th")
                        if not th:
                            continue
                        th_text = th.get_text(strip=True)
                        tds = r.find_all("td")
                        val = next((td.get_text(strip=True) for td in tds if td.get_text(strip=True)), "N/A")
                        if "PER" in th_text:
                            per = val
                        elif "PBR" in th_text:
                            pbr = val
                        elif "ROE" in th_text:
                            roe = val
                        elif "EPS" in th_text:
                            eps = val
                except Exception:
                    pass

            finance_results.append({"ì¢…ëª©ëª…": stock, "PER": per, "PBR": pbr, "ROE": roe, "EPS": eps, "í˜„ì¬ê°€": price})

        df_finance = pd.DataFrame(finance_results)
        if not df_finance.empty:
            df_finance["PER_f"] = df_finance["PER"].apply(safe_float)
            df_finance["ROE_f"] = df_finance["ROE"].apply(safe_float)
            df_finance["EPS_f"] = df_finance["EPS"].apply(safe_float)
            df_finance["í˜„ì¬ê°€_f"] = df_finance["í˜„ì¬ê°€"].apply(safe_float)
            df_finance["ì˜ˆìƒì£¼ê°€"] = df_finance["EPS_f"] * 10
            df_finance["ì˜ˆìƒì£¼ê°€_í‘œì‹œ"] = df_finance["ì˜ˆìƒì£¼ê°€"].apply(lambda x: f"{x:,.0f}" if pd.notnull(x) else "N/A")
            df_finance["ìƒìŠ¹ì—¬ë ¥"] = df_finance["ì˜ˆìƒì£¼ê°€"] - df_finance["í˜„ì¬ê°€_f"]
            
            def format_sign(x):
                if pd.isnull(x): return "N/A"
                if x > 0: return f"+{x:,.0f}"
                if x < 0: return f"{x:,.0f}"
                return "0"

            df_finance["ìƒìŠ¹ì—¬ë ¥_í‘œì‹œ"] = df_finance["ìƒìŠ¹ì—¬ë ¥"].apply(format_sign)
            df_finance["ROE"] = df_finance["ROE"].apply(lambda x: f"{x}%" if x != "N/A" and not str(x).endswith("%") else x)
            df_finance["í˜„ì¬ê°€_í‘œì‹œ"] = df_finance["í˜„ì¬ê°€_f"].apply(lambda x: f"{x:,.0f}" if pd.notnull(x) else "N/A")

        news_df["ì¢…ëª©ëª…"] = news_df["ì¢…ëª©ëª…"].astype(str)
        df_finance["ì¢…ëª©ëª…"] = df_finance["ì¢…ëª©ëª…"].astype(str)
        result_table = pd.merge(news_df, df_finance, on="ì¢…ëª©ëª…", how="left")
        result_table["ì˜¤ëŠ˜ê¸°ì‚¬"] = result_table["ë‰´ìŠ¤íŒë³„"]

        show_cols = ["ì¢…ëª©ëª…", "ë‰´ìŠ¤", "ìš”ì•½", "ë‰´ìŠ¤ë‚ ì§œ", "ë‰´ìŠ¤íŒë³„", "AIì„¤ëª…", "ì¹´í…Œê³ ë¦¬",
             "ì˜ˆìƒì£¼ê°€_í‘œì‹œ", "í˜„ì¬ê°€_í‘œì‹œ", "ìƒìŠ¹ì—¬ë ¥_í‘œì‹œ", "ì˜¤ëŠ˜ê¸°ì‚¬", "ë§í¬"]

        for col in ["ì˜ˆìƒì£¼ê°€_í‘œì‹œ", "í˜„ì¬ê°€_í‘œì‹œ", "ìƒìŠ¹ì—¬ë ¥_í‘œì‹œ"]:
            if col not in result_table.columns:
                result_table[col] = "N/A"

        final_df = result_table[show_cols].rename(columns={
            "ì˜ˆìƒì£¼ê°€_í‘œì‹œ": "ì˜ˆìƒì£¼ê°€", "í˜„ì¬ê°€_í‘œì‹œ": "í˜„ì¬ê°€", "ìƒìŠ¹ì—¬ë ¥_í‘œì‹œ": "ì˜ˆìƒì£¼ê°€-í˜„ì¬ê°€"
        })

        def highlight_news(row):
            return [color_news(row["ë‰´ìŠ¤íŒë³„"])] * len(row)

        st.write("### ğŸ“° ì˜¤ëŠ˜ ì¢…ëª© ë‰´ìŠ¤ ì „ì²´")
        st.dataframe(final_df.style.apply(highlight_news, axis=1), use_container_width=True)

        # í˜¸ì¬ ìƒìœ„ 5
        expecting_stocks = final_df[(final_df["ë‰´ìŠ¤íŒë³„"] == "í˜¸ì¬") & (final_df["ì˜ˆìƒì£¼ê°€-í˜„ì¬ê°€"] != "N/A")]
        def to_float(x):
            try: return float(str(x).replace(",", "").replace("+", ""))
            except Exception: return -9999999
        expecting_stocks_unique = expecting_stocks.sort_values("ì˜ˆìƒì£¼ê°€-í˜„ì¬ê°€", ascending=False).drop_duplicates(subset=["ì¢…ëª©ëª…"], keep="first")
        top5_expect = expecting_stocks_unique.head(5)
        if not top5_expect.empty:
            st.write("### ğŸš€ ê°€ì¥ ê¸°ëŒ€ë˜ëŠ” ì¢…ëª© TOP 5 (í˜¸ì¬ + ìƒìŠ¹ì—¬ë ¥ ë†’ì€ ìˆœ)")
            st.dataframe(top5_expect[["ì¢…ëª©ëª…","ë‰´ìŠ¤","ìš”ì•½","ë‰´ìŠ¤íŒë³„","AIì„¤ëª…","ì˜ˆìƒì£¼ê°€","í˜„ì¬ê°€","ì˜ˆìƒì£¼ê°€-í˜„ì¬ê°€","ë§í¬"]])
        else:
            st.info("í˜¸ì¬ì— í•´ë‹¹í•˜ëŠ” ì¢…ëª©ì´ ì—†ê±°ë‚˜ ìƒìŠ¹ì—¬ë ¥ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")

        # í˜¸ì¬ ì°¨íŠ¸
        st.write("## ğŸ“Š í˜¸ì¬ ì¢…ëª© 20ì¼ ë³¼ë¦°ì € ë°´ë“œ ì°¨íŠ¸")
        for idx, row in expecting_stocks_unique.iterrows():
            stock_name = row["ì¢…ëª©ëª…"]; code = get_code_from_name(stock_name)
            if not code:
                st.warning(f"{stock_name} ì¢…ëª©ì½”ë“œ ì—†ìŒ, ì°¨íŠ¸ ìƒëµ"); continue
            try:
                df_price = crawl_naver_daily_price(code, max_days=60)
            except Exception as e:
                st.warning(f"{stock_name} ì‹œì„¸ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}"); continue
            date_news_map = {}
            for _, nrow in news_df[news_df["ì¢…ëª©ëª…"] == stock_name].iterrows():
                dt = nrow["ë‰´ìŠ¤ë‚ ì§œ"]; date_news_map.setdefault(dt, []).append({"title": nrow["ë‰´ìŠ¤"], "link": nrow["ë§í¬"]})
            plot_bollinger_20day(df_price, stock_name, code, key=f"bollinger_good_{code}_{idx}", news_dict=date_news_map)

        # ì•…ì¬ ì°¨íŠ¸
        st.write("## ğŸ“Š ì•…ì¬ ì¢…ëª© 20ì¼ ë³¼ë¦°ì € ë°´ë“œ ì°¨íŠ¸")
        bad_stocks = final_df[(final_df["ë‰´ìŠ¤íŒë³„"] == "ì•…ì¬")]
        bad_stocks_unique = bad_stocks.drop_duplicates(subset=["ì¢…ëª©ëª…"], keep="first")
        for idx, row in bad_stocks_unique.iterrows():
            stock_name = row["ì¢…ëª©ëª…"]; code = get_code_from_name(stock_name)
            if not code:
                st.warning(f"{stock_name} ì¢…ëª©ì½”ë“œ ì—†ìŒ, ì°¨íŠ¸ ìƒëµ"); continue
            try:
                df_price = crawl_naver_daily_price(code, max_days=60)
            except Exception as e:
                st.warning(f"{stock_name} ì‹œì„¸ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}"); continue
            date_news_map = {}
            for _, nrow in news_df[news_df["ì¢…ëª©ëª…"] == stock_name].iterrows():
                dt = nrow["ë‰´ìŠ¤ë‚ ì§œ"]; date_news_map.setdefault(dt, []).append({"title": nrow["ë‰´ìŠ¤"], "link": nrow["ë§í¬"]})
            plot_bollinger_20day(df_price, stock_name, code, key=f"bollinger_bad_{code}_{idx}", news_dict=date_news_map)

        st.session_state.is_running = False
