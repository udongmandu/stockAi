import streamlit as st
import pandas as pd
import requests
from bs4 import BeautifulSoup
import time
import os
import json
from datetime import datetime, timedelta
from openai import OpenAI
import plotly.graph_objects as go
from dotenv import load_dotenv
import re
from concurrent.futures import ThreadPoolExecutor, as_completed

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

KRX_FILE = "krx_temp.xls"
CACHE_FILE = "cache_news_ai.json"

# ë‚ ì§œë³„ ì„¹ì…˜ ë‰´ìŠ¤(ì¼ë°˜ ëª¨ë“œ)
NEWS_BY_DATE_URL = "https://finance.naver.com/news/mainnews.naver?&date={date}&page={page}"

st.set_page_config(page_title="KRX ë‰´ìŠ¤-AI ìë™í™”", layout="centered")
st.title("KRX ìƒì¥ì¢…ëª© ë‰´ìŠ¤ + AI ë¶„ì„ ìë™í™”")

# -------------------- ì„¸ì…˜/ìºì‹œ --------------------
@st.cache_resource
def get_session():
    from requests.adapters import HTTPAdapter
    from urllib3.util.retry import Retry

    s = requests.Session()
    s.headers.update({
        "User-Agent": "Mozilla/5.0",
        "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
    })

    # ì»¤ë„¥ì…˜ í’€/ì¬ì‹œë„
    retry = Retry(
        total=3, backoff_factor=0.3,
        status_forcelist=[429, 500, 502, 503, 504],
        allowed_methods=["GET", "HEAD", "OPTIONS"]
    )
    adapter = HTTPAdapter(max_retries=retry, pool_connections=20, pool_maxsize=20)
    s.mount("http://", adapter)
    s.mount("https://", adapter)
    return s

SESSION = get_session()

@st.cache_data(ttl=3600)
def load_krx_excel(path: str) -> pd.DataFrame:
    try:
        try:
            df = pd.read_excel(path, dtype=str, engine="xlrd")
        except Exception:
            df = pd.read_excel(path, dtype=str)
        df["ì¢…ëª©ì½”ë“œ"] = df["ì¢…ëª©ì½”ë“œ"].apply(lambda x: str(x).zfill(6))
        return df
    except Exception as e:
        raise e

@st.cache_data(ttl=1800)
def crawl_naver_daily_price_cached(stock_code, max_days=60) -> pd.DataFrame:
    base_url = "https://finance.naver.com/item/sise_day.naver"
    all_rows, page = [], 1
    while True:
        params = {"code": stock_code, "page": page}
        res = SESSION.get(base_url, params=params, timeout=10)
        soup = BeautifulSoup(res.content, "lxml")
        table = soup.find("table", class_="type2")
        if not table:
            break
        rows = table.find_all("tr")
        data_rows = [row for row in rows if len(row.find_all("td")) == 7]
        if not data_rows:
            break
        for row in data_rows:
            cols = row.find_all("td")
            date = cols[0].get_text(strip=True).replace(".", "-")
            close = cols[1].get_text(strip=True).replace(",", "")
            open_price = cols[3].get_text(strip=True).replace(",", "")
            high = cols[4].get_text(strip=True).replace(",", "")
            low = cols[5].get_text(strip=True).replace(",", "")
            volume = cols[6].get_text(strip=True).replace(",", "")
            all_rows.append({
                "ë‚ ì§œ": date,
                "ì¢…ê°€": float(close) if close else None,
                "ì‹œê°€": float(open_price) if open_price else None,
                "ê³ ê°€": float(high) if high else None,
                "ì €ê°€": float(low) if low else None,
                "ê±°ë˜ëŸ‰": int(volume) if volume else None
            })
            if len(all_rows) >= max_days:
                break
        if len(all_rows) >= max_days:
            break
        page += 1
    df = pd.DataFrame(all_rows)
    df = df.dropna(subset=["ì¢…ê°€"])
    df["ë‚ ì§œ"] = pd.to_datetime(df["ë‚ ì§œ"])
    df = df.sort_values("ë‚ ì§œ").reset_index(drop=True)
    return df

# í™˜ê²½ë³€ìˆ˜ì—ì„œ API í‚¤ ì½ê¸°
api_key_env = os.getenv("OPENAI_API_KEY")
if "api_key" not in st.session_state:
    st.session_state.api_key = api_key_env

def submit_api_key():
    st.session_state.api_key = st.session_state.api_key_input

if st.session_state.api_key is None:
    st.text_input("API í‚¤ê°€ ìˆëŠ” ENV íŒŒì¼ì„ ë°›ì•„ ì£¼ì„¸ìš”.", type="password", key="api_key_input", on_change=submit_api_key)
else:
    st.success("âœ… ENV íŒŒì¼ ì¸ì‹ ì™„ë£Œ")

# ëª¨ë“œ í† ê¸€ ìƒíƒœ
if "specific_mode" not in st.session_state:
    st.session_state.specific_mode = False

# íŒŒì¼ ì²´í¬
file_exists = os.path.isfile(KRX_FILE)
if file_exists and (st.session_state.api_key is not None):
    st.success(f"âœ… '{KRX_FILE}' íŒŒì¼ì´ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
else:
    if not file_exists:
        st.warning(
            "âŒ '{KRX_FILE}' íŒŒì¼ì´ í´ë”ì— ì—†ìŠµë‹ˆë‹¤.\n"
            "https://kind.krx.co.kr/corpgeneral/corpList.do?method=download&searchType=13\n"
            "ìœ„ ì£¼ì†Œì—ì„œ íŒŒì¼ ë°›ì€ í›„\n"
            "íŒŒì¼ -> ë‚´ë³´ë‚´ê¸° -> íŒŒì¼ í˜•ì‹ ë³€ê²½ -> Excel 97 - 2003 í†µí•© ë¬¸ì„œë¡œ ë‚´ë³´ë‚´ê¸°\n"
            "'krx_temp.xls'ë¡œ ì´ë¦„ì„ ë³€ê²½í•˜ì—¬ ì´ íŒŒì´ì¬ íŒŒì¼ê³¼ ê°™ì€ í´ë”ì— ìœ„ì¹˜ì‹œì¼œ ì£¼ì„¸ìš”."
        )
    if st.session_state.api_key is None:
        st.warning("âŒ OpenAI API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")

# KRX ì¢…ëª© ë¡œë”©
if file_exists:
    try:
        df_stocklist = load_krx_excel(KRX_FILE)
        stock_names = sorted(set(df_stocklist["íšŒì‚¬ëª…"].dropna()))
    except Exception as e:
        st.error(f"ì—‘ì…€ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")
        stock_names = []
else:
    stock_names = []

def get_code_from_name(stock_name):
    try:
        matched_rows = df_stocklist[df_stocklist["íšŒì‚¬ëª…"] == stock_name]
        if not matched_rows.empty:
            code = matched_rows.iloc[0]["ì¢…ëª©ì½”ë“œ"]
            return str(code).zfill(6)
        return None
    except Exception:
        return None

# ---------- ë‚ ì§œ íŒŒì‹± (ì¼ë°˜ ëª¨ë“œ í•„ìš” ì‹œ) ----------
def parse_news_date(dt_text: str) -> datetime.date:
    raw = (dt_text or "").strip().replace(".", "-")
    token = raw.split()[0]  # 'YYYY-MM-DD' or 'MM-DD'
    parts = token.split("-")
    if len(parts) == 2:  # 'MM-DD' -> attach current year
        year = datetime.today().year
        token = f"{year}-{parts[0].zfill(2)}-{parts[1].zfill(2)}"
    return datetime.strptime(token, "%Y-%m-%d").date()

# ---------- íšŒì‚¬ë‰´ìŠ¤ ë‚ ì§œ íŒŒì‹± ----------
def parse_company_news_date(s: str) -> datetime.date:
    # ì˜ˆ: "2025.08.10 13:24"
    s = (s or "").strip()
    s = s.replace("-", ".").replace("/", ".")
    token = s.split()[0]  # "YYYY.MM.DD"
    return datetime.strptime(token, "%Y.%m.%d").date()

# ---------- í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬(ê³µë°± ì œê±° + ì†Œë¬¸ì) ----------
def normalize_text(s: str) -> str:
    if not isinstance(s, str):
        return ""
    return s.replace(" ", "").lower()

# ---------- ê³µí†µ ìœ í‹¸ ----------
def find_stock_in_text(text, stock_names_set):
    if not isinstance(text, str):
        return None
    for name in stock_names_set:
        if name in text:
            return name
    return None

def classify_news(title, summary):
    news_text = f"{title} {summary}"
    prompt = f"""ì•„ë˜ ë‰´ìŠ¤ê°€ í•´ë‹¹ ê¸°ì—…ì— í˜¸ì¬(ìƒìŠ¹ ê°€ëŠ¥ì„±), ì•…ì¬(í•˜ë½ ê°€ëŠ¥ì„±), ì¤‘ë¦½ ì¤‘ ì–´ë–¤ ì˜í–¥ì„ ë¯¸ì¹ ì§€ í•œê¸€ë¡œ ë‹¨ë‹µ(í˜¸ì¬/ì•…ì¬/ì¤‘ë¦½)ê³¼ ì´ìœ (1ë¬¸ì¥)ë¥¼ ì•Œë ¤ì¤˜.
ë‰´ìŠ¤: {news_text}"""
    try:
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=100,
            temperature=0
        )
        answer = response.choices[0].message.content.strip()
        if answer.startswith("í˜¸ì¬"):
            tag = "í˜¸ì¬"
        elif answer.startswith("ì•…ì¬"):
            tag = "ì•…ì¬"
        else:
            tag = "ì¤‘ë¦½"
        return tag, answer
    except Exception as e:
        return "ë¶„ì„ë¶ˆê°€", f"API ì˜¤ë¥˜: {e}"

def safe_float(val):
    try:
        return float(str(val).replace(",", ""))
    except Exception:
        return None

def crawl_naver_daily_price(stock_code, max_days=60):
    return crawl_naver_daily_price_cached(stock_code, max_days=max_days)

def plot_bollinger_20day(df_price, stock_name, stock_code, key=None, news_dict=None):
    if len(df_price) < 20:
        st.warning(f"{stock_name} ì‹œì„¸ ë°ì´í„°ê°€ ë¶€ì¡±í•´ ë³¼ë¦°ì € ë°´ë“œë¥¼ ê·¸ë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    window = 20
    df_price[f"MA{window}"] = df_price["ì¢…ê°€"].rolling(window=window).mean()
    df_price[f"STD{window}"] = df_price["ì¢…ê°€"].rolling(window=window).std()
    df_price[f"Upper{window}"] = df_price[f"MA{window}"] + 2 * df_price[f"STD{window}"]
    df_price[f"Lower{window}"] = df_price[f"MA{window}"] - 2 * df_price[f"STD{window}"]
    dates = df_price["ë‚ ì§œ"]; max_date = dates.max(); min_date = dates.min()
    st.write(f"### {stock_name} 20ì¼ ë³¼ë¦°ì € ë°´ë“œ")
    st.markdown(f"[ì£¼ê°€ ìƒì„¸ ë³´ê¸°](https://finance.naver.com/item/main.nhn?code={stock_code})", unsafe_allow_html=True)
    fig = go.Figure(
        data=[
            go.Scatter(x=dates, y=df_price["ì¢…ê°€"], mode="lines", name="ì¢…ê°€"),
            go.Scatter(x=dates, y=df_price[f"MA{window}"], mode="lines", name=f"MA{window}"),
            go.Scatter(x=dates, y=df_price[f"Upper{window}"], mode="lines", name="ìƒë‹¨ ë°´ë“œ"),
            go.Scatter(x=dates, y=df_price[f"Lower{window}"], mode="lines", name="í•˜ë‹¨ ë°´ë“œ", fill="tonexty", fillcolor="rgba(200,200,200,0.2)"),
        ],
        layout=go.Layout(
            xaxis_title="ë‚ ì§œ", yaxis_title="ê°€ê²©",
            xaxis=dict(range=[min_date, max_date + pd.Timedelta(days=3)], fixedrange=True),
            yaxis=dict(autorange=True, fixedrange=True),
            dragmode="pan"
        )
    )
    if news_dict:
        for nd, news_list in news_dict.items():
            try:
                nd_dt = pd.to_datetime(nd)
                price_row = df_price[df_price["ë‚ ì§œ"] == nd_dt]
                if not price_row.empty:
                    price = price_row.iloc[0]["ì¢…ê°€"]
                    news_text = "<br>".join([n["title"] for n in news_list])
                    fig.add_trace(go.Scatter(
                        x=[nd_dt], y=[price], mode="markers+text",
                        marker=dict(color="orange", size=7, symbol="circle"),
                        text=["ğŸ“°"], textposition="top center",
                        name=f"ë‰´ìŠ¤({nd})", hoverinfo="text", hovertext=news_text, showlegend=False
                    ))
            except Exception:
                pass
    st.plotly_chart(fig, use_container_width=True, key=key)

# ------------------- ëª¨ë“œ ì „í™˜ ë²„íŠ¼ (ë‹¨ì¼) -------------------
toggle_label = "â†© ì¼ë°˜ ëª¨ë“œë¡œ ì „í™˜" if st.session_state.specific_mode else "ğŸ“° íŠ¹ì • ê¸°ì‚¬ë§Œ ë³´ê¸° (ë¯¸ì™„)"
if st.button(toggle_label):
    st.session_state.specific_mode = not st.session_state.specific_mode

# ------------------- ì†ë„ ê°œì„  ë„ìš°ë¯¸ -------------------
@st.cache_data(ttl=600)
def get_last_page_for_date(date_param: str) -> int:
    """
    í•´ë‹¹ ë‚ ì§œ(date=YYYYMMDD)ì˜ mainnews ë§ˆì§€ë§‰ í˜ì´ì§€ ë²ˆí˜¸ë¥¼ ì¶”ì •.
    - 'ë§¨ë’¤' ë§í¬ê°€ ìˆìœ¼ë©´ ê·¸ page íŒŒë¼ë¯¸í„°
    - ì—†ìœ¼ë©´ í˜ì´ì§• ìˆ«ì ë§í¬ ì¤‘ ìµœëŒ€ê°’
    """
    url = NEWS_BY_DATE_URL.format(date=date_param, page=1)
    res = SESSION.get(url, timeout=10)
    soup = BeautifulSoup(res.content, "lxml")

    last_link = soup.select_one("td.pgRR a")
    if last_link and last_link.get("href"):
        m = re.search(r"page=(\d+)", last_link["href"])
        if m:
            return int(m.group(1))

    pages = []
    for a in soup.select("table.Nnavi a"):
        txt = (a.get_text(strip=True) or "").strip()
        if txt.isdigit():
            pages.append(int(txt))
    return max(pages) if pages else 1

def parse_mainnews_page(html_bytes, fallback_date_str: str, stock_key_norm: str):
    """
    mainnews í•œ í˜ì´ì§€ì—ì„œ í•´ë‹¹ ì¢…ëª© í‚¤ì›Œë“œê°€ ì œëª©/ìš”ì•½ì— ìˆìœ¼ë©´ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜
    """
    soup = BeautifulSoup(html_bytes, "lxml")
    items = soup.select("ul.newsList > li")
    results = []
    if not items:
        return results

    for li in items:
        a = li.select_one("dd.articleSubject a")
        sm = li.select_one("dd.articleSummary")
        if not a or not sm:
            continue

        title = a.get_text(strip=True)
        href = a.get("href", "")
        link = href if href.startswith("http") else "https://finance.naver.com" + href
        summary = sm.get_text(" ", strip=True)

        dt_tag = sm.select_one("span.wdate")
        if dt_tag:
            news_date_only = dt_tag.get_text(strip=True).split(" ")[0].replace(".", "-")
        else:
            news_date_only = fallback_date_str

        title_key = normalize_text(title)
        summary_key = normalize_text(summary)
        if (stock_key_norm in title_key) or (stock_key_norm in summary_key):
            results.append({
                "ì¢…ëª©ëª…": None,  # ë‚˜ì¤‘ì— ì±„ì›€
                "ë‰´ìŠ¤": title,
                "ë§í¬": link,
                "ìš”ì•½": summary,
                "ë‰´ìŠ¤ë‚ ì§œ": news_date_only,
            })
    return results

# ------------------- ëª¨ë“œë³„ UI/ë¡œì§ -------------------
if "is_running" not in st.session_state:
    st.session_state.is_running = False

# ===== íŠ¹ì • ê¸°ì‚¬ë§Œ ë³´ê¸° ëª¨ë“œ (ë‚ ì§œ ì„¹ì…˜ ì „ì²´ ìˆœíšŒ + ì¢…ëª©ëª… í•„í„°) =====
if st.session_state.specific_mode:
    st.subheader("ğŸ“° íŠ¹ì • ê¸°ì‚¬ë§Œ ë³´ê¸°")

    days_to_fetch = st.number_input("ê°€ì ¸ì˜¬ ë‰´ìŠ¤ ì¼ì (ìµœê·¼ Nì¼)", min_value=1, max_value=100, value=30, step=1)
    selected_stock = st.selectbox("ì¢…ëª© ì„ íƒ (ê²€ìƒ‰ ê°€ëŠ¥)", options=stock_names, index=0 if stock_names else None)

    can_run_specific = file_exists and (selected_stock is not None)
    run_specific = st.button("ğŸ” ë‰´ìŠ¤ ê²€ìƒ‰ (íŠ¹ì • ê¸°ì‚¬ë§Œ ë³´ê¸°) ì‹¤í–‰", disabled=not can_run_specific)

    def crawl_mainnews_by_dates_for_stock(stock_name: str, days: int = 30, max_pages_per_day: int = 200):
        """
        ì£¼ìš”ë‰´ìŠ¤(mainnews)ì—ì„œ date=YYYYMMDDì˜ 1..last_pageë¥¼ ë³‘ë ¬ í¬ë¡¤ë§.
        ê° liì—ì„œ ì œëª©/ìš”ì•½ ë‘˜ ë‹¤ì— stock_name(ì •ê·œí™”)ì´ ë“±ì¥í•˜ë©´ ìˆ˜ì§‘.
        """
        results = []
        today = datetime.today().date()
        stock_key_norm = normalize_text(stock_name)
        MAX_WORKERS = 12  # ë„¤íŠ¸ì›Œí¬/ë¨¸ì‹  í™˜ê²½ ë”°ë¼ 8~16 ì‚¬ì´ì—ì„œ ì¡°ì ˆ ì¶”ì²œ

        for i in range(days):
            d = today - timedelta(days=i)
            date_param = d.strftime("%Y%m%d")
            fallback_date_str = d.strftime("%Y-%m-%d")

            try:
                last_page = get_last_page_for_date(date_param)
                last_page = min(last_page, max_pages_per_day)
            except Exception as e:
                st.warning(f"[{date_param}] ë§ˆì§€ë§‰ í˜ì´ì§€ íŒŒì•… ì‹¤íŒ¨: {e} (1í˜ì´ì§€ë§Œ ì‹œë„)")
                last_page = 1

            page_indices = list(range(1, last_page + 1))

            # ë³‘ë ¬ í¬ë¡¤ë§
            page_results = []
            with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:
                future_to_page = {
                    ex.submit(
                        lambda p: parse_mainnews_page(
                            SESSION.get(NEWS_BY_DATE_URL.format(date=date_param, page=p), timeout=10).content,
                            fallback_date_str, stock_key_norm
                        ), page
                    ): page for page in page_indices
                }
                for fut in as_completed(future_to_page):
                    try:
                        part = fut.result() or []
                        page_results.extend(part)
                    except Exception:
                        # ê°œë³„ í˜ì´ì§€ ì‹¤íŒ¨ëŠ” ë¬´ì‹œ
                        pass

            # ì¢…ëª©ëª… ì±„ìš°ê³  í•©ì¹˜ê¸°
            for r in page_results:
                r["ì¢…ëª©ëª…"] = stock_name
            results.extend(page_results)

        # ì¤‘ë³µ ì œê±° + ì •ë ¬
        results = list({(r["ë‰´ìŠ¤"], r["ë‰´ìŠ¤ë‚ ì§œ"]): r for r in results}.values())
        results.sort(key=lambda x: (x["ë‰´ìŠ¤ë‚ ì§œ"], x["ë‰´ìŠ¤"]))
        return results

    if run_specific:
        st.info(f"ì„ íƒ ì¢…ëª©: {selected_stock} / ìµœê·¼ {int(days_to_fetch)}ì¼ ê¸°ì‚¬ ìˆ˜ì§‘ ì¤‘â€¦ (ê°€ì ¸ì˜¬ ë‰´ìŠ¤ì˜ ê°¯ìˆ˜ê°€ ë§ì„ ìˆ˜ë¡ ëŠë¦½ë‹ˆë‹¤ìš”)")
        news_list = crawl_mainnews_by_dates_for_stock(selected_stock, days=int(days_to_fetch))
        if len(news_list) == 0:
            st.warning("í•´ë‹¹ ê¸°ê°„ì— í•´ë‹¹ ì¢…ëª©ëª…ì´ í¬í•¨ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            df_sel = pd.DataFrame(news_list).drop_duplicates(["ì¢…ëª©ëª…", "ë‰´ìŠ¤"])
            st.write("### ğŸ“„ ìˆ˜ì§‘ ê¸°ì‚¬")
            st.dataframe(df_sel[["ë‰´ìŠ¤ë‚ ì§œ", "ë‰´ìŠ¤", "ë§í¬", "ìš”ì•½"]], use_container_width=True)

            # ë‰´ìŠ¤-ì°¨íŠ¸ ë§¤í•‘
            date_news_map = {}
            for _, nrow in df_sel.iterrows():
                dt = nrow["ë‰´ìŠ¤ë‚ ì§œ"]
                date_news_map.setdefault(dt, [])
                if not any(n["title"] == nrow["ë‰´ìŠ¤"] for n in date_news_map[dt]):
                    date_news_map[dt].append({"title": nrow["ë‰´ìŠ¤"], "link": nrow["ë§í¬"]})

            # ì°¨íŠ¸ìš© ê°€ê²© (ì¢…ëª©ì½”ë“œ í•„ìš”)
            code = get_code_from_name(selected_stock)
            if not code:
                st.warning(f"{selected_stock} ì¢…ëª©ì½”ë“œ ì—†ìŒ, ì°¨íŠ¸ ìƒëµ")
            else:
                try:
                    df_price = crawl_naver_daily_price(code, max_days=60)
                except Exception as e:
                    st.warning(f"{selected_stock} ì‹œì„¸ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                    df_price = None
                if df_price is not None:
                    st.write(f"## ğŸ—“ ì„ íƒ ì¢…ëª© ìµœê·¼ {int(days_to_fetch)}ì¼ ë‰´ìŠ¤ ì°¨íŠ¸")
                    plot_bollinger_20day(df_price, selected_stock, code,
                                         key=f"bollinger_specific_{code}_{days_to_fetch}",
                                         news_dict=date_news_map)

# ===== ì¼ë°˜ ëª¨ë“œ =====
else:
    news_count = st.number_input("ê°€ì ¸ì˜¬ ë‰´ìŠ¤ ê¸°ì‚¬ ê°œìˆ˜ ì…ë ¥ (í‘œì‹œìš©)", min_value=1, max_value=200, value=20, step=1)
    today_only = st.checkbox("ê¸ˆì¼ ê¸°ì‚¬ë§Œ", value=True)
    graph_show_in_cache = st.checkbox("ì§€ê¸ˆ ê¹Œì§€ ëª¨ë“  ë°ì´í„° ë³´ê¸°", value=False)

    start_btn_disabled = not (file_exists and st.session_state.api_key is not None)
    start = st.button("ğŸš€ ì‹œì‘ (ê¸°ë³¸ ëª¨ë“œ)", disabled=start_btn_disabled or st.session_state.is_running)

    def color_news(tag):
        if tag == "í˜¸ì¬":
            return "color: red; font-weight: bold;"
        elif tag == "ì•…ì¬":
            return "color: blue; font-weight: bold;"
        elif tag == "ì¤‘ë¦½":
            return "color: gray;"
        else:
            return ""

    # -------- ì¼ë°˜ ëª¨ë“œ: ë©”ì¸ë‰´ìŠ¤ ì „ì²´ í˜ì´ì§€ ìˆœíšŒ í¬ë¡¤ëŸ¬ --------
    def crawl_mainnews_all_pages(stock_names, today_only=True, max_pages=200):
        """
        ë„¤ì´ë²„ ê¸ˆìœµ ë©”ì¸ë‰´ìŠ¤ë¥¼ page=1ë¶€í„° ê¸°ì‚¬ ì—†ì„ ë•Œê¹Œì§€ ì „ë¶€ ìˆœíšŒ.
        ì œëª©/ìš”ì•½ì— ì¢…ëª©ëª…ì´ í¬í•¨ë˜ë©´ ìˆ˜ì§‘.
        today_only=Trueë©´ 'ì˜¤ëŠ˜ ë‚ ì§œ'ê°€ ì•„ë‹Œ ê¸°ì‚¬ë¶€í„°ëŠ” ì´í›„ í˜ì´ì§€ ìˆœíšŒ ì¡°ê¸° ì¢…ë£Œ.
        """
        results = []
        stock_set = set(stock_names)
        today_str = datetime.today().strftime("%Y-%m-%d")

        page = 1
        while True:
            news_url = f"https://finance.naver.com/news/mainnews.naver?&page={page}"
            try:
                res = SESSION.get(news_url, timeout=10)
                soup = BeautifulSoup(res.content, "lxml")
            except Exception as e:
                st.error(f"ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§ ì˜¤ë¥˜ (í˜ì´ì§€ {page}): {e}")
                return results  # ì‹¤íŒ¨ ì‹œ ì¡°ê¸° ë°˜í™˜

            items = soup.select("ul.newsList > li")
            if not items:
                return results  # ë” ì´ìƒ í˜ì´ì§€ ì—†ìŒ -> ì¦‰ì‹œ ë°˜í™˜

            hit_non_today = False  # ì˜¤ëŠ˜ë§Œ ë³´ê¸°ì¼ ë•Œ, ë¹„-ì˜¤ëŠ˜ ê¸°ì‚¬ ë§Œë‚˜ë©´ ì´í›„ í˜ì´ì§€ ì¤‘ë‹¨

            for li in items:
                a = li.select_one("dd.articleSubject a")
                sm = li.select_one("dd.articleSummary")
                dt_tag = li.select_one("dd.articleSummary span.wdate")
                if not (a and dt_tag):
                    continue

                # ë‚ ì§œ
                news_date_only = dt_tag.get_text(strip=True).split(" ")[0].replace(".", "-")

                # ì˜¤ëŠ˜ë§Œ ë³´ê¸°ë©´, ë¹„-ì˜¤ëŠ˜ ê¸°ì‚¬ë¶€í„°ëŠ” ì¢…ë£Œ í”Œë˜ê·¸
                if today_only and news_date_only != today_str:
                    hit_non_today = True
                    continue

                # ì œëª©/ìš”ì•½
                title = a.get_text(strip=True)
                link = a.get("href", "")
                link = link if link.startswith("http") else "https://finance.naver.com" + link
                summary = sm.get_text(" ", strip=True) if sm else ""

                # ì œëª©ê³¼ ìš”ì•½ ë‘˜ ë‹¤ ê²€ì‚¬
                stock = find_stock_in_text(title, stock_set) or find_stock_in_text(summary, stock_set)
                if stock:
                    results.append({
                        "ì¢…ëª©ëª…": stock,
                        "ë‰´ìŠ¤": title,
                        "ë§í¬": link,
                        "ìš”ì•½": summary,
                        "ë‰´ìŠ¤ë‚ ì§œ": news_date_only
                    })

            if today_only and hit_non_today:
                return results  # ì˜¤ëŠ˜ ê¸°ì‚¬ ëë‚¬ìœ¼ë‹ˆ ì¦‰ì‹œ ë°˜í™˜

            page += 1
            if page > max_pages:
                return results  # ì•ˆì „ì¥ì¹˜

    if start:
        st.session_state.is_running = True

        # â–¼â–¼â–¼ ì¼ë°˜ ëª¨ë“œ ìˆ˜ì§‘
        news_results = crawl_mainnews_all_pages(
            stock_names=stock_names,
            today_only=today_only,
            max_pages=200,
        )

        if len(news_results) == 0:
            st.warning("ë‰´ìŠ¤ì—ì„œ ìƒì¥ì¢…ëª©ëª…ì´ í¬í•¨ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            st.session_state.is_running = False
            st.stop()

        # í™”ë©´ í‘œì‹œìš©ìœ¼ë¡œ ìƒìœ„ news_countê°œë§Œ
        news_results_display = news_results[:news_count]
        news_df = pd.DataFrame(news_results_display).drop_duplicates(["ì¢…ëª©ëª…", "ë‰´ìŠ¤"])

        # ìºì‹œ
        if os.path.exists(CACHE_FILE):
            with open(CACHE_FILE, "r", encoding="utf-8") as f:
                cache_dict = json.load(f)
        else:
            cache_dict = {}

        openai_api_key = st.session_state.api_key
        client = OpenAI(api_key=openai_api_key)

        news_df["ë‰´ìŠ¤íŒë³„"] = ""
        news_df["AIì„¤ëª…"] = ""
        today_str = datetime.today().strftime("%Y-%m-%d")

        for idx, row in news_df.iterrows():
            cached = cache_dict.get(row["ë‰´ìŠ¤"])
            if cached:
                if today_only and cached.get("ë‰´ìŠ¤ë‚ ì§œ") != today_str:
                    tag, explanation = classify_news(row["ë‰´ìŠ¤"], row["ìš”ì•½"])
                    news_df.at[idx, "ë‰´ìŠ¤íŒë³„"] = tag
                    news_df.at[idx, "AIì„¤ëª…"] = explanation
                    st.info(f"[AIë¶„ì„]{row['ì¢…ëª©ëª…']}: {row['ë‰´ìŠ¤']} => {tag}")
                    time.sleep(0.05)
                    cache_dict[row["ë‰´ìŠ¤"]] = {"ë‰´ìŠ¤íŒë³„": tag, "AIì„¤ëª…": explanation, "ë‰´ìŠ¤ë‚ ì§œ": row["ë‰´ìŠ¤ë‚ ì§œ"]}
                else:
                    news_df.at[idx, "ë‰´ìŠ¤íŒë³„"] = cached["ë‰´ìŠ¤íŒë³„"]
                    news_df.at[idx, "AIì„¤ëª…"] = cached["AIì„¤ëª…"]
                    st.info(f"[ìºì‹œ] {row['ì¢…ëª©ëª…']}: {row['ë‰´ìŠ¤']} => {cached['ë‰´ìŠ¤íŒë³„']}")
            else:
                tag, explanation = classify_news(row["ë‰´ìŠ¤"], row["ìš”ì•½"])
                news_df.at[idx, "ë‰´ìŠ¤íŒë³„"] = tag
                news_df.at[idx, "AIì„¤ëª…"] = explanation
                st.info(f"[AIë¶„ì„]{row['ì¢…ëª©ëª…']}: {row['ë‰´ìŠ¤']} => {tag}")
                time.sleep(0.05)
                cache_dict[row["ë‰´ìŠ¤"]] = {"ë‰´ìŠ¤íŒë³„": tag, "AIì„¤ëª…": explanation, "ë‰´ìŠ¤ë‚ ì§œ": row["ë‰´ìŠ¤ë‚ ì§œ"]}

        with open(CACHE_FILE, "w", encoding="utf-8") as f:
            json.dump(cache_dict, f, ensure_ascii=False, indent=2)

        # ì¬ë¬´
        finance_results = []
        for stock in set(news_df["ì¢…ëª©ëª…"]):
            code = get_code_from_name(stock)
            if code is None:
                st.warning(f"{stock} ì¢…ëª©ì½”ë“œ ë¯¸ë°œê²¬, ì¬ë¬´ ë°ì´í„° ìƒëµ")
                continue
            url = f"https://finance.naver.com/item/main.nhn?code={code}"
            try:
                res = SESSION.get(url, timeout=10)
                soup = BeautifulSoup(res.content, "lxml")
            except Exception as e:
                st.warning(f"{stock} ì¬ë¬´ ë°ì´í„° ìš”ì²­ ì‹¤íŒ¨: {e}")
                continue

            price = "N/A"
            try:
                price_tag = soup.select_one("p.no_today span.blind")
                if price_tag:
                    price = price_tag.get_text(strip=True).replace(",", "")
            except Exception:
                pass

            per = pbr = roe = eps = "N/A"
            table = soup.select_one("table.tb_type1_ifrs")
            if table:
                try:
                    rows = table.select("tr")
                    for r in rows:
                        th = r.find("th")
                        if not th:
                            continue
                        th_text = th.get_text(strip=True)
                        tds = r.find_all("td")
                        val = next((td.get_text(strip=True) for td in tds if td.get_text(strip=True)), "N/A")
                        if "PER" in th_text:
                            per = val
                        elif "PBR" in th_text:
                            pbr = val
                        elif "ROE" in th_text:
                            roe = val
                        elif "EPS" in th_text:
                            eps = val
                except Exception:
                    pass

            finance_results.append({"ì¢…ëª©ëª…": stock, "PER": per, "PBR": pbr, "ROE": roe, "EPS": eps, "í˜„ì¬ê°€": price})

        df_finance = pd.DataFrame(finance_results)
        if not df_finance.empty:
            df_finance["PER_f"] = df_finance["PER"].apply(safe_float)
            df_finance["ROE_f"] = df_finance["ROE"].apply(safe_float)
            df_finance["EPS_f"] = df_finance["EPS"].apply(safe_float)
            df_finance["í˜„ì¬ê°€_f"] = df_finance["í˜„ì¬ê°€"].apply(safe_float)
            df_finance["ì˜ˆìƒì£¼ê°€"] = df_finance["EPS_f"] * 10
            df_finance["ì˜ˆìƒì£¼ê°€_í‘œì‹œ"] = df_finance["ì˜ˆìƒì£¼ê°€"].apply(lambda x: f"{x:,.0f}" if pd.notnull(x) else "N/A")
            df_finance["ìƒìŠ¹ì—¬ë ¥"] = df_finance["ì˜ˆìƒì£¼ê°€"] - df_finance["í˜„ì¬ê°€_f"]

            def format_sign(x):
                if pd.isnull(x): return "N/A"
                if x > 0: return f"+{x:,.0f}"
                if x < 0: return f"{x:,.0f}"
                return "0"

            df_finance["ìƒìŠ¹ì—¬ë ¥_í‘œì‹œ"] = df_finance["ìƒìŠ¹ì—¬ë ¥"].apply(format_sign)
            df_finance["ROE"] = df_finance["ROE"].apply(lambda x: f"{x}%" if x != "N/A" and not str(x).endswith("%") else x)
            df_finance["í˜„ì¬ê°€_í‘œì‹œ"] = df_finance["í˜„ì¬ê°€_f"].apply(lambda x: f"{x:,.0f}" if pd.notnull(x) else "N/A")

        news_df["ì¢…ëª©ëª…"] = news_df["ì¢…ëª©ëª…"].astype(str)
        df_finance["ì¢…ëª©ëª…"] = df_finance["ì¢…ëª©ëª…"].astype(str)
        result_table = pd.merge(news_df, df_finance, on="ì¢…ëª©ëª…", how="left")
        result_table["ì˜¤ëŠ˜ê¸°ì‚¬"] = result_table["ë‰´ìŠ¤íŒë³„"]

        show_cols = ["ì¢…ëª©ëª…", "ë‰´ìŠ¤", "ìš”ì•½", "ë‰´ìŠ¤ë‚ ì§œ", "ë‰´ìŠ¤íŒë³„", "AIì„¤ëª…",
                     "ì˜ˆìƒì£¼ê°€_í‘œì‹œ", "í˜„ì¬ê°€_í‘œì‹œ", "ìƒìŠ¹ì—¬ë ¥_í‘œì‹œ", "ì˜¤ëŠ˜ê¸°ì‚¬", "ë§í¬"]
        for col in ["ì˜ˆìƒì£¼ê°€_í‘œì‹œ", "í˜„ì¬ê°€_í‘œì‹œ", "ìƒìŠ¹ì—¬ë ¥_í‘œì‹œ"]:
            if col not in result_table.columns:
                result_table[col] = "N/A"

        final_df = result_table[show_cols].rename(columns={
            "ì˜ˆìƒì£¼ê°€_í‘œì‹œ": "ì˜ˆìƒì£¼ê°€", "í˜„ì¬ê°€_í‘œì‹œ": "í˜„ì¬ê°€", "ìƒìŠ¹ì—¬ë ¥_í‘œì‹œ": "ì˜ˆìƒì£¼ê°€-í˜„ì¬ê°€"
        })

        def highlight_news(row):
            return [color_news(row["ë‰´ìŠ¤íŒë³„"])] * len(row)

        st.write("### ğŸ“° ì˜¤ëŠ˜ ì¢…ëª© ë‰´ìŠ¤ ì „ì²´")
        st.dataframe(final_df.style.apply(highlight_news, axis=1), use_container_width=True)

        # í˜¸ì¬ ìƒìœ„ 5
        expecting_stocks = final_df[(final_df["ë‰´ìŠ¤íŒë³„"] == "í˜¸ì¬") & (final_df["ì˜ˆìƒì£¼ê°€-í˜„ì¬ê°€"] != "N/A")]
        def to_float(x):
            try: return float(str(x).replace(",", "").replace("+", ""))
            except Exception: return -9999999
        expecting_stocks_unique = expecting_stocks.sort_values("ì˜ˆìƒì£¼ê°€-í˜„ì¬ê°€", ascending=False)\
                                                 .drop_duplicates(subset=["ì¢…ëª©ëª…"], keep="first")
        top5_expect = expecting_stocks_unique.head(5)
        if not top5_expect.empty:
            st.write("### ğŸš€ ê°€ì¥ ê¸°ëŒ€ë˜ëŠ” ì¢…ëª© TOP 5 (í˜¸ì¬ + ìƒìŠ¹ì—¬ë ¥ ë†’ì€ ìˆœ)")
            st.dataframe(top5_expect[["ì¢…ëª©ëª…","ë‰´ìŠ¤","ìš”ì•½","ë‰´ìŠ¤íŒë³„","AIì„¤ëª…","ì˜ˆìƒì£¼ê°€","í˜„ì¬ê°€","ì˜ˆìƒì£¼ê°€-í˜„ì¬ê°€","ë§í¬"]])
        else:
            st.info("í˜¸ì¬ì— í•´ë‹¹í•˜ëŠ” ì¢…ëª©ì´ ì—†ê±°ë‚˜ ìƒìŠ¹ì—¬ë ¥ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")

        # í˜¸ì¬ ì°¨íŠ¸
        st.write("## ğŸ“Š í˜¸ì¬ ì¢…ëª© 20ì¼ ë³¼ë¦°ì € ë°´ë“œ ì°¨íŠ¸")
        for idx, row in expecting_stocks_unique.iterrows():
            stock_name = row["ì¢…ëª©ëª…"]; code = get_code_from_name(stock_name)
            if not code:
                st.warning(f"{stock_name} ì¢…ëª©ì½”ë“œ ì—†ìŒ, ì°¨íŠ¸ ìƒëµ"); continue
            try:
                df_price = crawl_naver_daily_price(code, max_days=60)
            except Exception as e:
                st.warning(f"{stock_name} ì‹œì„¸ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}"); continue
            date_news_map = {}
            for _, nrow in news_df[news_df["ì¢…ëª©ëª…"] == stock_name].iterrows():
                dt = nrow["ë‰´ìŠ¤ë‚ ì§œ"]; date_news_map.setdefault(dt, []).append({"title": nrow["ë‰´ìŠ¤"], "link": nrow["ë§í¬"]})
            if os.path.exists(CACHE_FILE):
                for news_text, val in cache_dict.items():
                    dt = val.get("ë‰´ìŠ¤ë‚ ì§œ")
                    if dt and (stock_name in news_text):
                        date_news_map.setdefault(dt, [])
                        if not any(n["title"] == news_text for n in date_news_map[dt]):
                            date_news_map[dt].append({"title": news_text, "link": None})
            plot_bollinger_20day(df_price, stock_name, code, key=f"bollinger_good_{code}_{idx}", news_dict=date_news_map)

        # ì•…ì¬ ì°¨íŠ¸
        st.write("## ğŸ“Š ì•…ì¬ ì¢…ëª© 20ì¼ ë³¼ë¦°ì € ë°´ë“œ ì°¨íŠ¸")
        bad_stocks = final_df[(final_df["ë‰´ìŠ¤íŒë³„"] == "ì•…ì¬")]
        bad_stocks_unique = bad_stocks.drop_duplicates(subset=["ì¢…ëª©ëª…"], keep="first")
        for idx, row in bad_stocks_unique.iterrows():
            stock_name = row["ì¢…ëª©ëª…"]; code = get_code_from_name(stock_name)
            if not code:
                st.warning(f"{stock_name} ì¢…ëª©ì½”ë“œ ì—†ìŒ, ì°¨íŠ¸ ìƒëµ"); continue
            try:
                df_price = crawl_naver_daily_price(code, max_days=60)
            except Exception as e:
                st.warning(f"{stock_name} ì‹œì„¸ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}"); continue
            date_news_map = {}
            for _, nrow in news_df[news_df["ì¢…ëª©ëª…"] == stock_name].iterrows():
                dt = nrow["ë‰´ìŠ¤ë‚ ì§œ"]; date_news_map.setdefault(dt, []).append({"title": nrow["ë‰´ìŠ¤"], "link": nrow["ë§í¬"]})
            if os.path.exists(CACHE_FILE):
                for news_text, val in cache_dict.items():
                    dt = val.get("ë‰´ìŠ¤ë‚ ì§œ")
                    if dt and (stock_name in news_text):
                        date_news_map.setdefault(dt, [])
                        if not any(n["title"] == news_text for n in date_news_map[dt]):
                            date_news_map[dt].append({"title": news_text, "link": None})
            plot_bollinger_20day(df_price, stock_name, code, key=f"bollinger_bad_{code}_{idx}", news_dict=date_news_map)

        # ìºì‹œ ì „ì²´ ë³´ê¸°
        if graph_show_in_cache:
            st.write("## ì§€ê¸ˆê¹Œì§€ì˜ ë°ì´í„°")
            cached_rows = []
            for news_title, val in cache_dict.items():
                stock = find_stock_in_text(news_title, set(stock_names))
                if stock:
                    cached_rows.append({
                        "ì¢…ëª©ëª…": stock, "ë‰´ìŠ¤": news_title, "ë§í¬": None, "ìš”ì•½": None,
                        "ë‰´ìŠ¤ë‚ ì§œ": val.get("ë‰´ìŠ¤ë‚ ì§œ"), "ë‰´ìŠ¤íŒë³„": val.get("ë‰´ìŠ¤íŒë³„", "ë¶„ì„ë¶ˆê°€"), "AIì„¤ëª…": val.get("AIì„¤ëª…", "")
                    })
            if len(cached_rows) == 0:
                st.info("ìºì‹œì— ì¢…ëª©ëª…ì´ í¬í•¨ëœ ì €ì¥ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            else:
                cached_df = pd.DataFrame(cached_rows).drop_duplicates(["ì¢…ëª©ëª…","ë‰´ìŠ¤"])
                def _color(tag):
                    if tag == "í˜¸ì¬": return "color: red; font-weight: bold;"
                    if tag == "ì•…ì¬": return "color: blue; font-weight: bold;"
                    if tag == "ì¤‘ë¦½": return "color: gray;"
                    return ""
                st.write("### ğŸ—‚ ìºì‹œëœ ì „ì²´ ë‰´ìŠ¤")
                st.dataframe(
                    cached_df[["ì¢…ëª©ëª…","ë‰´ìŠ¤","ë‰´ìŠ¤ë‚ ì§œ","ë‰´ìŠ¤íŒë³„","AIì„¤ëª…"]].style.apply(
                        lambda row: [_color(row["ë‰´ìŠ¤íŒë³„"])]*5, axis=1
                    ),
                    use_container_width=True
                )
                for idx, stock_name in enumerate(cached_df["ì¢…ëª©ëª…"].dropna().unique().tolist()):
                    code = get_code_from_name(stock_name)
                    if not code:
                        st.warning(f"{stock_name} ì¢…ëª©ì½”ë“œ ì—†ìŒ, ì°¨íŠ¸ ìƒëµ"); continue
                    try:
                        df_price = crawl_naver_daily_price(code, max_days=60)
                    except Exception as e:
                        st.warning(f"{stock_name} ì‹œì„¸ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}"); continue
                    date_news_map = {}
                    subset = cached_df[cached_df["ì¢…ëª©ëª…"] == stock_name]
                    for _, nrow in subset.iterrows():
                        dt = nrow["ë‰´ìŠ¤ë‚ ì§œ"]
                        if dt:
                            date_news_map.setdefault(dt, [])
                            if not any(n["title"] == nrow["ë‰´ìŠ¤"] for n in date_news_map[dt]):
                                date_news_map[dt].append({"title": nrow["ë‰´ìŠ¤"], "link": None})
                    plot_bollinger_20day(df_price, stock_name, code, key=f"bollinger_cache_{code}_{idx}", news_dict=date_news_map)

        st.session_state.is_running = False
